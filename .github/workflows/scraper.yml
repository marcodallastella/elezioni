# name: scraper

# on:
#   workflow_dispatch:
#   # schedule:
#   #   - cron: '*/5 * * * *'

# jobs:
#   data_processing:
#     runs-on: ubuntu-latest
#     steps:
#       - name: Checkout repository
#         uses: actions/checkout@v2

#       - name: Set up Python
#         uses: actions/setup-python@v2
#         with:
#           python-version: '3.10'

#       - name: Install dependencies
#         run: |
#           pip install -r requirements.txt

#       - name: Look for new data UMBRIA
#         run: jupyter nbconvert --to notebook --execute "notebooks/3.2_risultati_um.ipynb"       
        
#       - name: Look for new data EMILIA ROMAGNA
#         run: jupyter nbconvert --to notebook --execute "notebooks/3.1_risultati_er.ipynb"  
        
#       # - name: Clean UMBRIA
#       #   run: jupyter nbconvert --to notebook --execute "notebooks/4.0_risultati_viz_er.ipynb"       
        
#       # - name: Clean EMILIA ROMAGNA
#       #   run: jupyter nbconvert --to notebook notebooks/4.1_risultati_viz_um.ipynb

#       - name: Commit and push changes
#         env:
#           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
#         run: |
#           git config user.name "Automated"
#           git config user.email "actions@users.noreply.github.com"
#           git add .
#           git commit -m "Automated data processing"
#           git push